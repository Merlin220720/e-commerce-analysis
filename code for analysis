import sys
import subprocess
import os
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import davies_bouldin_score
from sklearn.neighbors import NearestNeighbors
import requests

# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning)

# Set environment variable to prevent MKL memory leak issue
os.environ["OMP_NUM_THREADS"] = "1"

# Install missing packages
def install_missing_packages():
    required_packages = ['pandas', 'numpy', 'matplotlib', 'seaborn', 'scikit-learn', 'requests']
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])

install_missing_packages()

# Download CSV files
def download_file(url, filename):
    response = requests.get(url)
    with open(filename, 'wb') as file:
        file.write(response.content)

files = {
    "Customers.csv": "https://drive.google.com/uc?export=download&id=1bu_--mo79VdUG9oin4ybfFGRUSXAe-WE",
    "Products.csv": "https://drive.google.com/uc?export=download&id=1IKuDizVapw-hyktwfpoAoaGtHtTNHfd0",
    "Transactions.csv": "https://drive.google.com/uc?export=download&id=1saEqdbBB-vuk2hxoAf4TzDEsykdKlzbF"
}

for filename, url in files.items():
    if not os.path.exists(filename):
        download_file(url, filename)

# Load Data
customers = pd.read_csv("Customers.csv")
products = pd.read_csv("Products.csv")
transactions = pd.read_csv("Transactions.csv")

# Ensure 'TotalValue' is computed in the transactions data
transactions['TotalValue'] = transactions['Quantity'] * transactions['Price']

# Exploratory Data Analysis (EDA)
def perform_eda():
    print("Customers Data Overview:\n", customers.head())
    print("Products Data Overview:\n", products.head())
    print("Transactions Data Overview:\n", transactions.head())
    
    # Checking for missing values
    print("Missing values in Customers:", customers.isnull().sum())
    print("Missing values in Products:", products.isnull().sum())
    print("Missing values in Transactions:", transactions.isnull().sum())
    
    # Visualizations
    plt.figure(figsize=(10, 5))
    sns.countplot(x='Region', data=customers)
    plt.title("Customer Distribution by Region")
    plt.show()

    plt.figure(figsize=(10, 5))
    top_products = transactions.groupby('ProductID')['TotalValue'].sum().sort_values(ascending=False).head(10)
    sns.barplot(x=top_products.index, y=top_products.values)
    plt.title("Top 10 Products by Total Revenue")
    plt.xlabel("ProductID")
    plt.ylabel("Total Revenue")
    plt.show()

perform_eda()

# Lookalike Model
def build_lookalike_model():
    merged_df = transactions.merge(customers, on='CustomerID').merge(products, on='ProductID')
    pivot_table = merged_df.pivot_table(index='CustomerID', columns='ProductID', values='Quantity', fill_value=0)
    
    # Nearest Neighbors Model
    model = NearestNeighbors(metric='cosine', algorithm='brute')
    model.fit(pivot_table)
    
    lookalike_dict = {}
    for cust in customers['CustomerID'][:20]:
        if cust in pivot_table.index:
            distances, indices = model.kneighbors(pivot_table.loc[[cust]].values, n_neighbors=4)
            lookalike_dict[cust] = [(pivot_table.index[i], distances[0][j]) for j, i in enumerate(indices[0][1:])]
    
    lookalike_df = pd.DataFrame(
        [(k, v[0][0], v[0][1], v[1][0], v[1][1], v[2][0], v[2][1]) for k, v in lookalike_dict.items()],
        columns=['CustomerID', 'Lookalike1', 'Score1', 'Lookalike2', 'Score2', 'Lookalike3', 'Score3']
    )
    lookalike_df.to_csv("Lookalike.csv", index=False)
    print("Lookalike Model Completed!")

build_lookalike_model()

# Customer Segmentation
def perform_clustering():
    # Aggregate customer spending
    customer_spending = transactions.groupby('CustomerID')[['TotalValue']].sum()
    customer_spending['TransactionCount'] = transactions.groupby('CustomerID')['TransactionID'].count()
    
    # Filter out customers with no spending
    customer_spending = customer_spending[customer_spending['TotalValue'] > 0]
    
    # Scaling the spending data
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(customer_spending)
    
    best_k = None
    best_db_index = float('inf')
    
    # Try KMeans for different k values and compute Davies-Bouldin score
    for k in range(2, 11):
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(data_scaled)
        db_index = davies_bouldin_score(data_scaled, labels)
        if db_index < best_db_index:
            best_db_index = db_index
            best_k = k
    
    # Final clustering with optimal k
    final_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
    clusters = final_kmeans.fit_predict(data_scaled)
    
    customer_spending['Cluster'] = clusters
    
    # Plot the clustering result
    plt.figure(figsize=(10, 5))
    sns.scatterplot(x=customer_spending.index, y=customer_spending['TotalValue'], hue=customer_spending['Cluster'], palette='viridis')
    plt.title(f"Customer Segmentation (Optimal Clusters: {best_k})")
    plt.show()
    
    print(f"Optimal Clusters: {best_k}, DB Index: {best_db_index}")

perform_clustering()
